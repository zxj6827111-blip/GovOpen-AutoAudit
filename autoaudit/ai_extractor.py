import os
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass, field
from datetime import datetime

# âœ… åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿.envä¸­çš„API KEYè¢«è¯»å–ï¼‰
from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)


# å°è¯•å¯¼å…¥AI Provider
try:
    from openai import OpenAI
    MODELSCOPE_AVAILABLE = True
except ImportError:
    logger.warning("openai not installed. ModelScope providers will be disabled.")
    MODELSCOPE_AVAILABLE = False


@dataclass
class AiInvocation:
    """AIè°ƒç”¨è®°å½•"""
    invocation_id: str
    provider: str  # "glm" | "deepseek"
    model: str
    prompt_version: str = "v1.0"
    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + "Z")
    latency_ms: int = 0
    input_tokens: int = 0
    output_tokens: int = 0
    total_tokens: int = 0
    success: bool = False
    error: Optional[str] = None
    result: Optional[Dict] = None


class AIExtractor:
    """AIè¾…åŠ©å­—æ®µæå– - æ”¯æŒDeepSeek/Qwen/GLMä¸‰ä¸ªProvider"""
    
    def __init__(
        self, 
        primary_provider="deepseek",  # DeepSeekä½œä¸ºé»˜è®¤ï¼ˆç»¼åˆæœ€ä¼˜ï¼‰
        fallback_provider="qwen",      # Qwenä½œä¸ºå¤‡é€‰ï¼ˆæœ€å¿«å“åº”ï¼‰
        max_tokens=2000,
        timeout_seconds=30,
        max_cost_per_batch=None  # ä»ç¯å¢ƒå˜é‡è¯»å–
    ):
        self.primary_provider = primary_provider
        self.fallback_provider = fallback_provider
        self.max_tokens = max_tokens
        self.timeout_seconds = timeout_seconds
        
        # âœ… ä»ç¯å¢ƒå˜é‡è¯»å–tokené™é¢ï¼Œé»˜è®¤50000ï¼ˆè¶³å¤Ÿå¤æ ¸å¤§é‡è§„åˆ™ï¼‰
        if max_cost_per_batch is None:
            max_cost_per_batch = int(os.environ.get("AI_MAX_TOKENS_PER_BATCH", "50000"))
        self.max_cost_per_batch = max_cost_per_batch
        
        # å½“å‰æ‰¹æ¬¡tokenæ¶ˆè€—
        self.batch_tokens_used = 0
        
        # AIè°ƒç”¨è®°å½•
        self.invocations: List[AiInvocation] = []
        
        # åˆå§‹åŒ–Providers
        self.deepseek_client = None
        self.qwen_client = None
        self.glm_client = None
        
        # ModelScope API Keyï¼ˆæ‰€æœ‰æ¨¡å‹å…±ç”¨ï¼‰
        modelscope_key = os.environ.get("DEEPSEEK_API_KEY")
        
        # DeepSeekï¼ˆé­”æ­ï¼‰åˆå§‹åŒ– - ä¸»è¦Provider
        if MODELSCOPE_AVAILABLE and modelscope_key:
            try:
                self.deepseek_client = OpenAI(
                    api_key=modelscope_key,
                    base_url="https://api-inference.modelscope.cn/v1"
                )
                logger.info("DeepSeek provider initialized (ModelScope) - Primary")
            except Exception as e:
                logger.error(f"Failed to initialize DeepSeek: {e}")
        
        # Qwen3-32Bï¼ˆé­”æ­ï¼‰åˆå§‹åŒ– - å¤‡ç”¨Provider
        if MODELSCOPE_AVAILABLE and modelscope_key:
            try:
                self.qwen_client = OpenAI(
                    api_key=modelscope_key,
                    base_url="https://api-inference.modelscope.cn/v1"
                )
                logger.info("Qwen3-32B provider initialized (ModelScope) - Fallback")
            except Exception as e:
                logger.error(f"Failed to initialize Qwen: {e}")
        
        # GLM-4.7ï¼ˆé­”æ­ï¼‰åˆå§‹åŒ– - ç‰¹æ®Šåœºæ™¯
        if MODELSCOPE_AVAILABLE and modelscope_key:
            try:
                self.glm_client = OpenAI(
                    api_key=modelscope_key,
                    base_url="https://api-inference.modelscope.cn/v1"
                )
                logger.info("GLM-4.7 provider initialized (ModelScope) - Special Cases")
            except Exception as e:
                logger.error(f"Failed to initialize GLM: {e}")

    
    def extract_fields(self, html_body: str, fields: List[str]) -> Dict[str, Optional[str]]:
        """
        ä»HTMLä¸­æå–æŒ‡å®šå­—æ®µï¼ˆæ”¯æŒåŒProviderï¼‰
        
        Args:
            html_body: é¡µé¢HTMLå†…å®¹
            fields: è¦æå–çš„å­—æ®µåˆ—è¡¨ï¼Œå¦‚["phone", "address"]
        
        Returns:
            {"phone": "025-12345", "address": "å—äº¬å¸‚..."}
        """
        # Cost Controlæ£€æŸ¥
        if self.batch_tokens_used >= self.max_cost_per_batch:
            logger.warning(f"Batch token limit reached ({self.batch_tokens_used}/{self.max_cost_per_batch}), skipping AI extraction")
            return {field: None for field in fields}
        
        # å°è¯•ä¸»Provider
        result = self._try_provider(self.primary_provider, html_body, fields)
        if result:
            return result
        
        # é™çº§åˆ°å‰¯Provider
        logger.warning(f"Primary provider {self.primary_provider} failed, trying fallback {self.fallback_provider}")
        result = self._try_provider(self.fallback_provider, html_body, fields)
        if result:
            return result
        
        # æ‰€æœ‰Provideréƒ½å¤±è´¥
        logger.error("All AI providers failed")
        return {field: None for field in fields}
    
    def _try_provider(self, provider: str, html_body: str, fields: List[str]) -> Optional[Dict]:
        """å°è¯•ä½¿ç”¨æŒ‡å®šProvider"""
        if provider == "deepseek":
            return self._extract_with_deepseek(html_body, fields)
        elif provider == "qwen":
            return self._extract_with_qwen(html_body, fields)
        elif provider == "glm":
            return self._extract_with_glm(html_body, fields)
        else:
            logger.error(f"Unknown provider: {provider}")
            return None
    
    def _extract_with_glm(self, html_body: str, fields: List[str]) -> Optional[Dict]:
        """ä½¿ç”¨GLM-4.7æå–"""
        if not self.glm_client:
            logger.warning("GLM client not available")
            return None
        
        invocation = AiInvocation(
            invocation_id=f"glm_{int(time.time()*1000)}",
            provider="glm",
            model="ZhipuAI/GLM-4.7"
        )
        
        try:
            prompt = self._build_extraction_prompt(html_body, fields)
            start_time = time.time()
            
            # GLMè°ƒç”¨ï¼ˆéæµå¼ï¼‰
            response = self.glm_client.chat.completions.create(
                model="ZhipuAI/GLM-4.7",  # ModelScope Model-Id
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=self.max_tokens,
                stream=False  # éæµå¼
            )
            
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            # è§£æç»“æœ
            result_text = response.choices[0].message.content.strip()
            result_text = self._clean_json_response(result_text)
            
            import json
            extracted = json.loads(result_text)
            
            # è®°å½•æˆåŠŸè°ƒç”¨
            invocation.latency_ms = elapsed_ms
            invocation.success = True
            invocation.result = extracted
            invocation.input_tokens = response.usage.prompt_tokens
            invocation.output_tokens = response.usage.completion_tokens
            invocation.total_tokens = response.usage.total_tokens
            
            self.batch_tokens_used += invocation.total_tokens
            self.invocations.append(invocation)
            
            logger.info(f"GLM extraction successful ({elapsed_ms}ms, {invocation.total_tokens} tokens)")
            return extracted
            
        except Exception as e:
            invocation.success = False
            invocation.error = str(e)
            self.invocations.append(invocation)
            logger.error(f"GLM extraction failed: {e}")
            return None
    
    def _extract_with_qwen(self, html_body: str, fields: List[str]) -> Optional[Dict]:
        """ä½¿ç”¨Qwen3-32Bæå–ï¼ˆæ”¯æŒthinkingæ¨¡å¼ï¼‰"""
        if not self.qwen_client:
            logger.warning("Qwen client not available")
            return None
        
        invocation = AiInvocation(
            invocation_id=f"qwen_{int(time.time()*1000)}",
            provider="qwen",
            model="Qwen/Qwen3-32B"
        )
        
        try:
            prompt = self._build_extraction_prompt(html_body, fields)
            start_time = time.time()
            
            # Qwen3è°ƒç”¨ï¼ˆéæµå¼ï¼Œæ˜¾å¼ç¦ç”¨thinkingï¼‰
            response = self.qwen_client.chat.completions.create(
                model="Qwen/Qwen3-32B",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=self.max_tokens,
                stream=False,  # éæµå¼
                extra_body={"enable_thinking": False}  # æ˜¾å¼ç¦ç”¨thinking
            )
            
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            # è§£æç»“æœ
            result_text = response.choices[0].message.content.strip()
            result_text = self._clean_json_response(result_text)
            
            import json
            extracted = json.loads(result_text)
            
            # è®°å½•æˆåŠŸè°ƒç”¨
            invocation.latency_ms = elapsed_ms
            invocation.success = True
            invocation.result = extracted
            invocation.input_tokens = response.usage.prompt_tokens
            invocation.output_tokens = response.usage.completion_tokens
            invocation.total_tokens = response.usage.total_tokens
            
            self.batch_tokens_used += invocation.total_tokens
            self.invocations.append(invocation)
            
            logger.info(f"Qwen extraction successful ({elapsed_ms}ms, {invocation.total_tokens} tokens)")
            return extracted
            
        except Exception as e:
            invocation.success = False
            invocation.error = str(e)
            self.invocations.append(invocation)
            logger.error(f"Qwen extraction failed: {e}")
            return None
    
    
    def _extract_with_deepseek(self, html_body: str, fields: List[str]) -> Optional[Dict]:
        """ä½¿ç”¨DeepSeekæå–ï¼ˆé­”æ­ç¤¾åŒºï¼‰"""
        if not self.deepseek_client:
            logger.warning("DeepSeek client not available")
            return None
        
        invocation = AiInvocation(
            invocation_id=f"deepseek_{int(time.time()*1000)}",
            provider="deepseek",
            model="deepseek-ai/DeepSeek-V3.2"  # é­”æ­ModelScope Model-Id
        )
        
        try:
            prompt = self._build_extraction_prompt(html_body, fields)
            start_time = time.time()
            
            # é­”æ­DeepSeekè°ƒç”¨ï¼ˆéæµå¼ï¼‰
            response = self.deepseek_client.chat.completions.create(
                model="deepseek-ai/DeepSeek-V3.2",  # ModelScope Model-Id
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=self.max_tokens,
                stream=False  # éæµå¼
            )
            
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            # è§£æç»“æœ
            result_text = response.choices[0].message.content.strip()
            result_text = self._clean_json_response(result_text)
            
            import json
            extracted = json.loads(result_text)
            
            # è®°å½•æˆåŠŸè°ƒç”¨
            invocation.latency_ms = elapsed_ms
            invocation.success = True
            invocation.result = extracted
            invocation.input_tokens = response.usage.prompt_tokens
            invocation.output_tokens = response.usage.completion_tokens
            invocation.total_tokens = response.usage.total_tokens
            
            self.batch_tokens_used += invocation.total_tokens
            self.invocations.append(invocation)
            
            logger.info(f"DeepSeek (é­”æ­) extraction successful ({elapsed_ms}ms, {invocation.total_tokens} tokens)")
            return extracted
            
        except Exception as e:
            invocation.success = False
            invocation.error = str(e)
            self.invocations.append(invocation)
            logger.error(f"DeepSeek extraction failed: {e}")
            return None
    
    def _build_extraction_prompt(self, html_body: str, fields: List[str]) -> str:
        """æ„å»ºæå–prompt"""
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_body, 'html.parser')
        for script in soup(["script", "style"]):
            script.decompose()
        text = soup.get_text(separator="\n", strip=True)
        
        # é™åˆ¶é•¿åº¦
        text = text[:5000]
        
        field_descriptions = {
            "phone": "è”ç³»ç”µè¯ï¼ˆå¦‚ï¼š025-12345678æˆ–010-12345678ï¼‰",
            "address": "åŠå…¬åœ°å€ï¼ˆå¦‚ï¼šæ±Ÿè‹çœå—äº¬å¸‚ç„æ­¦åŒºXXè·¯XXå·ï¼‰",
            "email": "ç”µå­é‚®ä»¶",
            "fax": "ä¼ çœŸå·ç "
        }
        
        fields_str = "\n".join([f"- {field}: {field_descriptions.get(field, field)}" for field in fields])
        
        return f"""ä»ä»¥ä¸‹æ”¿åºœç½‘ç«™å†…å®¹ä¸­æå–æŒ‡å®šå­—æ®µã€‚

å†…å®¹:
{text}

éœ€è¦æå–çš„å­—æ®µ:
{fields_str}

è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œå¦‚:
{{"phone": "025-12345678", "address": "æ±Ÿè‹çœå—äº¬å¸‚ç„æ­¦åŒº..."}}

å¦‚æœæŸä¸ªå­—æ®µæ‰¾ä¸åˆ°ï¼Œè¿”å›nullã€‚åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
    
    def _clean_json_response(self, text: str) -> str:
        """æ¸…ç†JSONå“åº”ï¼ˆç§»é™¤markdownæ ‡è®°ï¼‰"""
        if text.startswith("```json"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        return text.strip()
    
    def review_uncertain_rule(self, rule: Dict, pages: List[Dict], reason: str) -> Dict:
        """
        ä½¿ç”¨AIå¤æ ¸UNCERTAINè§„åˆ™
        
        Args:
            rule: è§„åˆ™å®šä¹‰
            pages: æ‰€æœ‰é¡µé¢å†…å®¹ï¼ˆå³ä½¿locatoræœªåŒ¹é…ï¼‰
            reason: UNCERTAINåŸå› ï¼ˆå¦‚ "no_pages_matched"ï¼‰
        
        Returns:
            {
                "status": "PASS" | "FAIL" | "UNCERTAIN",
                "confidence": float,  # 0.0-1.0
                "reasoning": str,     # AIåˆ¤æ–­ç†ç”±
                "suggested_action": str  # å»ºè®®æ“ä½œ
            }
        """
        # Cost Controlæ£€æŸ¥
        if self.batch_tokens_used >= self.max_cost_per_batch:
            logger.warning(f"Batch token limit reached, skipping AI review")
            return {
                "status": "UNCERTAIN",
                "confidence": 0.0,
                "reasoning": "Tokené™é¢å·²è¾¾ä¸Šé™ï¼Œæ— æ³•è¿›è¡ŒAIå¤æ ¸",
                "suggested_action": "increase_token_limit"
            }
        
        # å°è¯•ä¸»Provider
        result = self._try_review_provider(self.primary_provider, rule, pages, reason)
        if result:
            return result
        
        # é™çº§åˆ°å‰¯Provider
        logger.warning(f"Primary provider {self.primary_provider} failed for review, trying fallback")
        result = self._try_review_provider(self.fallback_provider, rule, pages, reason)
        if result:
            return result
        
        # æ‰€æœ‰Provideréƒ½å¤±è´¥
        logger.error("All AI providers failed for review")
        return {
            "status": "UNCERTAIN",
            "confidence": 0.0,
            "reasoning": "AIå¤æ ¸å¤±è´¥",
            "suggested_action": "manual_review"
        }
    
    def _try_review_provider(self, provider: str, rule: Dict, pages: List[Dict], reason: str) -> Optional[Dict]:
        """å°è¯•ä½¿ç”¨æŒ‡å®šProviderè¿›è¡Œå¤æ ¸"""
        if provider == "deepseek":
            return self._review_with_deepseek(rule, pages, reason)
        elif provider == "qwen":
            return self._review_with_qwen(rule, pages, reason)
        elif provider == "glm":
            return self._review_with_glm(rule, pages, reason)
        else:
            logger.error(f"Unknown provider: {provider}")
            return None
    
    def _review_with_deepseek(self, rule: Dict, pages: List[Dict], reason: str) -> Optional[Dict]:
        """ä½¿ç”¨DeepSeekå¤æ ¸UNCERTAINè§„åˆ™"""
        if not self.deepseek_client:
            logger.warning("DeepSeek client not available")
            return None
        
        invocation = AiInvocation(
            invocation_id=f"deepseek_review_{int(time.time()*1000)}",
            provider="deepseek",
            model="deepseek-ai/DeepSeek-V3.2"
        )
        
        try:
            prompt = self._build_review_prompt(rule, pages, reason)
            start_time = time.time()
            
            response = self.deepseek_client.chat.completions.create(
                model="deepseek-ai/DeepSeek-V3.2",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ”¿åŠ¡å…¬å¼€è¯„ä¼°ä¸“å®¶ï¼Œè´Ÿè´£å¤æ ¸ä¸ç¡®å®šçš„è§„åˆ™åˆ¤å®šã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1000,
                stream=False
            )
            
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            # è§£æç»“æœ
            result_text = response.choices[0].message.content.strip()
            result_text = self._clean_json_response(result_text)
            
            import json
            review_result = json.loads(result_text)
            
            # è®°å½•æˆåŠŸè°ƒç”¨
            invocation.latency_ms = elapsed_ms
            invocation.success = True
            invocation.result = review_result
            invocation.input_tokens = response.usage.prompt_tokens
            invocation.output_tokens = response.usage.completion_tokens
            invocation.total_tokens = response.usage.total_tokens
            
            self.batch_tokens_used += invocation.total_tokens
            self.invocations.append(invocation)
            
            logger.info(f"DeepSeek review successful: {review_result['status']} (confidence: {review_result['confidence']:.2f})")
            return review_result
            
        except Exception as e:
            invocation.success = False
            invocation.error = str(e)
            self.invocations.append(invocation)
            logger.error(f"DeepSeek review failed: {e}")
            return None
    
    def _review_with_qwen(self, rule: Dict, pages: List[Dict], reason: str) -> Optional[Dict]:
        """ä½¿ç”¨Qwenå¤æ ¸UNCERTAINè§„åˆ™"""
        if not self.qwen_client:
            return None
        
        invocation = AiInvocation(
            invocation_id=f"qwen_review_{int(time.time()*1000)}",
            provider="qwen",
            model="Qwen/Qwen3-32B"
        )
        
        try:
            prompt = self._build_review_prompt(rule, pages, reason)
            start_time = time.time()
            
            response = self.qwen_client.chat.completions.create(
                model="Qwen/Qwen3-32B",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ”¿åŠ¡å…¬å¼€è¯„ä¼°ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1000,
                stream=False,
                extra_body={"enable_thinking": False}
            )
            
            elapsed_ms = int((time.time() - start_time) * 1000)
            result_text = response.choices[0].message.content.strip()
            result_text = self._clean_json_response(result_text)
            
            import json
            review_result = json.loads(result_text)
            
            invocation.latency_ms = elapsed_ms
            invocation.success = True
            invocation.result = review_result
            invocation.input_tokens = response.usage.prompt_tokens
            invocation.output_tokens = response.usage.completion_tokens
            invocation.total_tokens = response.usage.total_tokens
            
            self.batch_tokens_used += invocation.total_tokens
            self.invocations.append(invocation)
            
            logger.info(f"Qwen review successful: {review_result['status']} (confidence: {review_result['confidence']:.2f})")
            return review_result
            
        except Exception as e:
            invocation.success = False
            invocation.error = str(e)
            self.invocations.append(invocation)
            logger.error(f"Qwen review failed: {e}")
            return None
    
    def _review_with_glm(self, rule: Dict, pages: List[Dict], reason: str) -> Optional[Dict]:
        """ä½¿ç”¨GLMå¤æ ¸UNCERTAINè§„åˆ™"""
        if not self.glm_client:
            return None
        
        invocation = AiInvocation(
            invocation_id=f"glm_review_{int(time.time()*1000)}",
            provider="glm",
            model="ZhipuAI/GLM-4.7"
        )
        
        try:
            prompt = self._build_review_prompt(rule, pages, reason)
            start_time = time.time()
            
            response = self.glm_client.chat.completions.create(
                model="ZhipuAI/GLM-4.7",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ”¿åŠ¡å…¬å¼€è¯„ä¼°ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1000,
                stream=False
            )
            
            elapsed_ms = int((time.time() - start_time) * 1000)
            result_text = response.choices[0].message.content.strip()
            result_text = self._clean_json_response(result_text)
            
            import json
            review_result = json.loads(result_text)
            
            invocation.latency_ms = elapsed_ms
            invocation.success = True
            invocation.result = review_result
            invocation.input_tokens = response.usage.prompt_tokens
            invocation.output_tokens = response.usage.completion_tokens
            invocation.total_tokens = response.usage.total_tokens
            
            self.batch_tokens_used += invocation.total_tokens
            self.invocations.append(invocation)
            
            logger.info(f"GLM review successful: {review_result['status']} (confidence: {review_result['confidence']:.2f})")
            return review_result
            
        except Exception as e:
            invocation.success = False
            invocation.error = str(e)
            self.invocations.append(invocation)
            logger.error(f"GLM review failed: {e}")
            return None
    
    def _build_review_prompt(self, rule: Dict, pages: List[Dict], reason: str) -> str:
        """æ„å»ºAIå¤æ ¸prompt"""
        from bs4 import BeautifulSoup
        
        # æå–æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬å†…å®¹ï¼ˆæœ€å¤š3ä¸ªé¡µé¢ï¼‰
        page_texts = []
        for page in pages[:3]:
            soup = BeautifulSoup(page.get("body", ""), 'html.parser')
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text(separator="\n", strip=True)
            page_texts.append(text[:2000])  # æ¯ä¸ªé¡µé¢æœ€å¤š2000å­—ç¬¦
        
        combined_text = "\n\n---\n\n".join(page_texts)
        
        # æ„å»ºè§„åˆ™æè¿°
        rule_desc = rule.get("description", "")
        locator = rule.get("locator", {})
        evaluator = rule.get("evaluator", {})
        
        locator_keywords = locator.get("keywords", [])
        evaluator_keywords = evaluator.get("keywords", [])
        
        return f"""ä½ æ˜¯æ”¿åŠ¡å…¬å¼€è¯„ä¼°ä¸“å®¶ã€‚ä¸€æ¡è§„åˆ™è¢«æ ‡è®°ä¸ºUNCERTAINï¼ˆä¸ç¡®å®šï¼‰ï¼Œéœ€è¦ä½ å¤æ ¸ã€‚

**è§„åˆ™æè¿°**: {rule_desc}

**UNCERTAINåŸå› **: {reason}

**è§„åˆ™è¦æ±‚**:
- å®šä½å…³é”®è¯: {locator_keywords}
- è¯„ä¼°å…³é”®è¯: {evaluator_keywords}

**ç½‘ç«™å†…å®¹æ‘˜è¦**:
{combined_text}

**ä»»åŠ¡**: 
åŸºäºä¸Šè¿°ç½‘ç«™å†…å®¹ï¼Œåˆ¤æ–­è¯¥è§„åˆ™åº”è¯¥æ˜¯PASSï¼ˆé€šè¿‡ï¼‰è¿˜æ˜¯FAILï¼ˆå¤±è´¥ï¼‰ï¼Œè¿˜æ˜¯ç¡®å®UNCERTAINï¼ˆæ— æ³•åˆ¤æ–­ï¼‰ã€‚

è¯·è¿”å›JSONæ ¼å¼:
{{
    "status": "PASS" æˆ– "FAIL" æˆ– "UNCERTAIN",
    "confidence": 0.0åˆ°1.0ä¹‹é—´çš„æ•°å­—ï¼ˆç½®ä¿¡åº¦ï¼Œå¦‚0.85è¡¨ç¤º85%ç¡®å®šï¼‰,
    "reasoning": "ä½ çš„åˆ¤æ–­ç†ç”±ï¼Œç”¨ä¸­æ–‡ç®€è¦è¯´æ˜ï¼ˆ1-2å¥è¯ï¼‰",
    "suggested_action": "å»ºè®®çš„æ“ä½œï¼Œå¦‚manual_reviewï¼ˆäººå·¥å¤æ ¸ï¼‰ã€add_keywordsï¼ˆæ·»åŠ å…³é”®è¯ï¼‰ç­‰"
}}

æ³¨æ„:
- åªæœ‰confidence > 0.8æ—¶æ‰å»ºè®®æ”¹å˜çŠ¶æ€ä¸ºPASSæˆ–FAIL
- å¦‚æœconfidence <= 0.8ï¼Œåº”ä¿æŒUNCERTAIN
- reasoningè¦å…·ä½“ï¼ŒæŒ‡å‡ºåœ¨å“ªé‡Œæ‰¾åˆ°ï¼ˆæˆ–æœªæ‰¾åˆ°ï¼‰ç›¸å…³å†…å®¹
"""
    
    def get_invocation_stats(self) -> Dict:
        """è·å–AIè°ƒç”¨ç»Ÿè®¡"""
        total = len(self.invocations)
        success = sum(1 for inv in self.invocations if inv.success)
        
        total_tokens = sum(inv.total_tokens for inv in self.invocations)
        avg_latency = sum(inv.latency_ms for inv in self.invocations) / total if total > 0 else 0
        
        provider_stats = {}
        for inv in self.invocations:
            if inv.provider not in provider_stats:
                provider_stats[inv.provider] = {"total": 0, "success": 0}
            provider_stats[inv.provider]["total"] += 1
            if inv.success:
                provider_stats[inv.provider]["success"] += 1
        
        return {
            "total_invocations": total,
            "successful_invocations": success,
            "success_rate": success / total if total > 0 else 0,
            "total_tokens_used": total_tokens,
            "batch_tokens_remaining": self.max_cost_per_batch - self.batch_tokens_used,
            "average_latency_ms": int(avg_latency),
            "provider_stats": provider_stats
        }
    
    def generate_audit_report(self) -> str:
        """ç”ŸæˆAIå®¡è®¡æŠ¥å‘Šï¼ˆMarkdownï¼‰"""
        stats = self.get_invocation_stats()
        
        md = []
        md.append("# AIè°ƒç”¨å®¡è®¡æŠ¥å‘Š\n\n")
        md.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.utcnow().isoformat()}Z\n\n")
        
        md.append("## ğŸ“Š è°ƒç”¨ç»Ÿè®¡\n\n")
        md.append(f"- **æ€»è°ƒç”¨æ¬¡æ•°**: {stats['total_invocations']}\n")
        md.append(f"- **æˆåŠŸæ¬¡æ•°**: {stats['successful_invocations']}\n")
        md.append(f"- **æˆåŠŸç‡**: {stats['success_rate']:.1%}\n")
        md.append(f"- **Tokenæ¶ˆè€—**: {stats['total_tokens_used']} / {self.max_cost_per_batch}\n")
        md.append(f"- **å¹³å‡å»¶è¿Ÿ**: {stats['average_latency_ms']}ms\n\n")
        
        md.append("## ğŸ”Œ Providerç»Ÿè®¡\n\n")
        md.append("| Provider | è°ƒç”¨æ¬¡æ•° | æˆåŠŸæ¬¡æ•° | æˆåŠŸç‡ |\n")
        md.append("|----------|----------|----------|--------|\n")
        for provider, ps in stats['provider_stats'].items():
            rate = ps['success'] / ps['total'] if ps['total'] > 0 else 0
            md.append(f"| {provider} | {ps['total']} | {ps['success']} | {rate:.1%} |\n")
        md.append("\n")
        
        md.append("## ğŸ“‹ è¯¦ç»†è°ƒç”¨è®°å½•\n\n")
        md.append("| æ—¶é—´ | Provider | å»¶è¿Ÿ | Tokens | çŠ¶æ€ |\n")
        md.append("|------|----------|------|--------|------|\n")
        for inv in self.invocations[-50:]:  # æœ€å¤šæ˜¾ç¤º50æ¡
            status = "âœ…" if inv.success else f"âŒ {inv.error[:30]}"
            md.append(f"| {inv.timestamp} | {inv.provider} | {inv.latency_ms}ms | {inv.total_tokens} | {status} |\n")
        
        return "".join(md)
